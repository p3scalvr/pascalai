from flask import Flask, render_template, request, jsonify, send_from_directory, Response
import ollama

app = Flask(__name__, template_folder='templates')

# Function to interact with Ollama's AI model
def get_ai_response(prompt: str):
    try:
        # Send a request to the Ollama model and get the response
        response = ollama.chat(model="llama3.2:3b", messages=[{"role": "user", "content": prompt}])

        # Check if the 'message' field is present and contains 'content'
        if "message" in response and "content" in response["message"]:
            ai_reply = response["message"]["content"]
        else:
            ai_reply = f"Error: Missing 'content' field in response. Full response: {response}"
        
        return ai_reply

    except Exception as e:
        # Handle any unexpected errors
        print(f"An error occurred: {e}")
        return f"An error occurred: {e}"
    
@app.route("/set-model/<model_name>", methods=["POST"])
def set_model(model_name):
    global current_model
    try:
        # Check if the requested model is installed
        installed_models = ollama.list_models()  # You may need a method to check for installed models
        if model_name not in installed_models:
            return jsonify({"success": False, "error": f"Model {model_name} not installed."}), 400
        
        current_model = model_name
        return jsonify({"success": True, "model": model_name})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route("/")
def home():
    # Serves the homepage (homePage.html)
    return render_template("homePage.html")  

@app.route('/static/<path:path>')
def send_static(path):
    # Serve static files
    return send_from_directory('static', path)

@app.route("/chat", methods=["POST"])
def chat():
    if request.method == "POST":
        user_input = request.json.get("prompt")
        if user_input:
            ai_response = get_ai_response(user_input)
            return jsonify({"response": ai_response})
        else:
            return jsonify({"response": "No prompt received."})
    elif request.method == "GET":
        return jsonify({"response": "GET method not supported for this endpoint."})
    
    @app.route("/chat-stream", methods=["POST"])
    def chat_stream():
        if request.method == "POST":
            user_input = request.json.get("prompt")
        
        def generate_stream(prompt):
            try:
                # Stream responses as they're generated by Ollama
                for chunk in ollama.stream_chat(
                    model="llama3.2:3b",
                    messages=[{"role": "user", "content": prompt}]
                ):
                    if "content" in chunk:  # Only send text content
                        yield f"data: {chunk['content']}\n\n"
            except Exception as e:
                yield f"data: [Error] {str(e)}\n\n"

        return Response(generate_stream(user_input), content_type='text/event-stream')
    return jsonify({"response": "Error: Invalid method."})

@app.route("/switch-model/<model_name>", methods=["GET"])
def switch_model(model_name):
    try:
        # Check if the model is installed
        available_models = ollama.models()
        if model_name not in available_models:
            return jsonify({"success": False, "message": f"Model {model_name} not found."})

        # Update the current model (perhaps store it in a global variable or database)
        current_model = model_name
        print(f"Switched to {current_model}")
        
        return jsonify({"success": True, "message": f"Model {model_name} is now active."})

    except Exception as e:
        return jsonify({"success": False, "message": str(e)})
    
@app.route("/chat-page")
def chat_page():
    # Serves the AI chat page (index.html)
    return render_template("index.html")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)